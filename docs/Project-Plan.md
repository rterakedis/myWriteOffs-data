# myWriteOffs-data Project Plan

## Overview

This repository hosts the **public valuation data** for the myWriteOffs application. It provides a version-controlled, transparent source of donation item valuations compiled from public IRS-recommended sources (Goodwill, Salvation Army, IRS Publication 561).

The data is structured to:

- Support the app's 3-layer data model (Base â†’ Cache â†’ User)
- Enable periodic updates without requiring app releases
- Preserve user customizations during data updates
- Maintain legal compliance and patent safety through public source attribution

## Repository Purpose

- **Primary Function**: Host JSON data files containing item categories, valuations, and metadata
- **Update Frequency**: Quarterly (January, April, July, October) as a best effort attempt to keep data current and relevant for tax purposes
- **Access Pattern**: Public read-only via GitHub raw URLs
- **Version Control**: Git tags for each data release (v1.0.0, v1.1.0, etc.)
- **Legal Compliance**: All data sourced from public guides with full attribution

## Project Structure

```
myWriteOffs-data/
â”œâ”€â”€ README.md                    # Repository overview and usage
â”œâ”€â”€ CHANGELOG.md                 # Version history and changes
â”œâ”€â”€ LICENSE                      # Data license (CC0 or similar)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Project-Plan.md         # This file
â”‚   â”œâ”€â”€ data-sources.md         # Detailed source documentation
â”‚   â”œâ”€â”€ scraping-guide.md       # How to update data
â”‚   â””â”€â”€ validation-rules.md     # Data quality standards
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ version.json            # Current version metadata
â”‚   â”œâ”€â”€ categories/
â”‚   â”‚   â”œâ”€â”€ clothing.json
â”‚   â”‚   â”œâ”€â”€ household.json
â”‚   â”‚   â”œâ”€â”€ electronics.json
â”‚   â”‚   â”œâ”€â”€ furniture.json
â”‚   â”‚   â”œâ”€â”€ books-media.json
â”‚   â”‚   â””â”€â”€ sports-recreation.json
â”‚   â”œâ”€â”€ items/
â”‚   â”‚   â”œâ”€â”€ clothing/
â”‚   â”‚   â”‚   â”œâ”€â”€ mens-suits.json
â”‚   â”‚   â”‚   â”œâ”€â”€ mens-shirts.json
â”‚   â”‚   â”‚   â”œâ”€â”€ womens-dresses.json
â”‚   â”‚   â”‚   â”œâ”€â”€ womens-blouses.json
â”‚   â”‚   â”‚   â””â”€â”€ shoes.json
â”‚   â”‚   â”œâ”€â”€ household/
â”‚   â”‚   â”‚   â”œâ”€â”€ kitchenware.json
â”‚   â”‚   â”‚   â”œâ”€â”€ linens.json
â”‚   â”‚   â”‚   â””â”€â”€ small-appliances.json
â”‚   â”‚   â”œâ”€â”€ electronics/
â”‚   â”‚   â”‚   â”œâ”€â”€ computers.json
â”‚   â”‚   â”‚   â””â”€â”€ audio-video.json
â”‚   â”‚   â””â”€â”€ furniture/
â”‚   â”‚       â”œâ”€â”€ living-room.json
â”‚   â”‚       â””â”€â”€ bedroom.json
â”‚   â””â”€â”€ metadata/
â”‚       â”œâ”€â”€ sources.json        # Source attribution
â”‚       â”œâ”€â”€ condition-terms.json # IRS-approved condition terms
â”‚       â””â”€â”€ valuation-methods.json
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scrape-goodwill.py      # Goodwill data scraper
â”‚   â”œâ”€â”€ parse-salvation-army.py # Salvation Army PDF parser
â”‚   â”œâ”€â”€ validate-data.py        # Data validation script
â”‚   â”œâ”€â”€ generate-version.py     # Version file generator
â”‚   â””â”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ archive/
â”‚   â”œâ”€â”€ 2024-Q1/
â”‚   â”œâ”€â”€ 2024-Q2/
â”‚   â””â”€â”€ ...
â””â”€â”€ tests/
    â”œâ”€â”€ test_data_integrity.py
    â””â”€â”€ test_json_schema.py
```

## Technical Constraints

- All scripts must be written in Python 3.8+ with minimal dependencies (called from requirements.txt)
- Scripts must include proper error handling and logging
- All data must be validated against JSON schemas before being committed
- Scripts must be idempotent and safe to run multiple times
- All scripts must be compatible with the CI/CD pipeline
- All scripts must be documented with docstrings and inline comments
- No other languages (node.js, typscript, etc) -- only Python allowed for all automation scripts
- All automation scripts must be self-contained and not require external build steps

## Data Schema

### version.json

```json
{
  "version": "1.0.0",
  "releaseDate": "2024-01-15T00:00:00Z",
  "changelog": "Initial release with Goodwill and Salvation Army valuations",
  "dataFiles": {
    "categories": "https://raw.githubusercontent.com/yourorg/myWriteOffs-data/main/data/categories/",
    "items": "https://raw.githubusercontent.com/yourorg/myWriteOffs-data/main/data/items/"
  },
  "sources": [
    {
      "id": "goodwill",
      "name": "Goodwill Valuation Guide",
      "lastUpdated": "2024-01-15",
      "url": "https://www.goodwill.org/donate/donation-value-guide/"
    },
    {
      "id": "salvation-army",
      "name": "Salvation Army Valuation Guide",
      "lastUpdated": "2024-01-15",
      "url": "https://satruck.org/Home/DonationValueGuide"
    },
    {
      "id": "irs-pub-561",
      "name": "IRS Publication 561",
      "lastUpdated": "2024-01-15",
      "url": "https://www.irs.gov/pub/irs-pdf/p561.pdf"
    }
  ],
  "minimumAppVersion": "1.0.0",
  "totalItems": 500,
  "totalCategories": 25
}
```

### Category File (e.g., clothing.json)

```json
{
  "id": "clothing",
  "name": "Clothing",
  "description": "Apparel and accessories for men, women, and children",
  "parent": null,
  "subcategories": [
    "mens-clothing",
    "womens-clothing",
    "childrens-clothing",
    "shoes",
    "accessories"
  ],
  "icon": "tshirt",
  "sortOrder": 1
}
```

### Item File (e.g., mens-suits.json)

```json
{
  "items": [
    {
      "id": "mens-suit-001",
      "name": "Men's Suit",
      "description": "Two-piece suit (jacket and pants)",
      "category": "mens-clothing",
      "tags": ["formal", "business", "professional"],
      "valuations": [
        {
          "sourceId": "goodwill",
          "value": 25.00,
          "condition": "good",
          "notes": "Clean, no stains or tears"
        },
        {
          "sourceId": "salvation-army",
          "value": 30.00,
          "condition": "good",
          "notes": "Good condition, minor wear acceptable"
        }
      ],
      "recommendedValue": 27.50,
      "valuationMethod": "Thrift Shop Value",
      "unit": "each",
      "lastUpdated": "2024-01-15T00:00:00Z"
    }
  ]
}
```

## Phase Breakdown

### Phase 1: Repository Setup & Infrastructure
**Goal:** Establish repository structure and basic tooling

- âœ… Create public GitHub repository `myWriteOffs-data`
- âœ… Initialize with README, LICENSE (CC-BY 4.0 for data)
- âœ… Set up directory structure (data/, scripts/, archive/, tests/)
- âœ… Create `.gitignore` for Python and temporary files
- âœ… Set up GitHub Actions for automated validation (PRs to main only)
- âœ… Create issue templates for data corrections
- ðŸ”³ Set up branch protection rules (require PR reviews) - *Requires GitHub remote*
- âœ… Create initial CHANGELOG.md
- âœ… Document contribution guidelines

**Deliverables:**
- âœ… Functional repository with proper structure
- âœ… Minimal GitHub Actions workflow for cost-effective validation
- âœ… Clear documentation for contributors

**Status:** Phase 1 Complete (except branch protection which requires GitHub remote)

---

### Phase 2: Data Collection & Scraping Scripts
**Goal:** Build tools to collect valuation data from public sources

#### 2.1: Goodwill Scraper
- ðŸ”³ Research Goodwill website structure and data format
- ðŸ”³ Create Python scraper for Goodwill Valuation Guide
- ðŸ”³ Parse HTML tables into structured data
- ðŸ”³ Extract item names, categories, and valuations
- ðŸ”³ Handle pagination and category navigation
- ðŸ”³ Add error handling and retry logic
- ðŸ”³ Save raw scraped data with timestamps

#### 2.2: Salvation Army Parser
- ðŸ”³ Download Salvation Army Valuation Guide PDF
- ðŸ”³ Create PDF parser using PyPDF2 or pdfplumber
- ðŸ”³ Extract tables and text data
- ðŸ”³ Parse item names and valuations
- ðŸ”³ Handle multi-page tables
- ðŸ”³ Normalize data format to match Goodwill structure
- ðŸ”³ Save parsed data with source attribution

#### 2.3: IRS Publication 561 Parser
- ðŸ”³ Download IRS Publication 561 PDF
- ðŸ”³ Extract relevant guidelines and examples
- ðŸ”³ Parse condition definitions and valuation methods
- ðŸ”³ Create metadata files for condition terms
- ðŸ”³ Document IRS-approved valuation approaches

#### 2.4: Data Normalization
- ðŸ”³ Create unified data schema for all sources
- ðŸ”³ Build normalization script to standardize formats
- ðŸ”³ Map source-specific categories to unified taxonomy
- ðŸ”³ Handle unit conversions (each, pair, set)
- ðŸ”³ Calculate recommended values (average/median)
- ðŸ”³ Add source attribution to each valuation

**Deliverables:**
- Working scrapers for all data sources
- Normalized JSON data files
- Source attribution metadata

---

### Phase 3: Data Validation & Quality Assurance
**Goal:** Ensure data integrity and consistency

#### 3.1: Local Validation Scripts
- ðŸ”³ Create comprehensive Python validation scripts for local development
- ðŸ”³ Create JSON schema definitions for all data types
- ðŸ”³ Build validation script to check schema compliance
- ðŸ”³ Validate required fields (id, name, valuations)
- ðŸ”³ Check value ranges (no negative prices)
- ðŸ”³ Verify category references are valid
- ðŸ”³ Ensure source IDs match sources.json
- ðŸ”³ Check for duplicate item IDs
- ðŸ”³ Validate date formats and timestamps
- ðŸ”³ Add command-line options for quick vs. full validation
- ðŸ”³ Create pre-commit hook integration for immediate feedback

#### 3.2: Data Quality Checks
- ðŸ”³ Flag items with missing valuations
- ðŸ”³ Identify outliers (values >3 std deviations)
- ðŸ”³ Check for inconsistent naming conventions
- ðŸ”³ Verify all items have categories
- ðŸ”³ Ensure descriptions are present and meaningful
- ðŸ”³ Validate URL formats in sources
- ðŸ”³ Check for broken category hierarchies

#### 3.3: GitHub Actions (Minimal, Cost-Effective)
- ðŸ”³ Set up GitHub Actions to run only on PRs to main branch (not on pushes)
- ðŸ”³ Configure minimal validation workflow (JSON schema only) to reduce costs
- ðŸ”³ Add PR template with manual validation checklist
- ðŸ”³ Create branch protection requiring PR review and validation passing
- ðŸ”³ Document cost-conscious CI/CD approach in repository documentation

#### 3.4: Local Testing
- ðŸ”³ Create unit tests for validation functions
- ðŸ”³ Test edge cases (empty files, malformed JSON)
- ðŸ”³ Test with sample data from each source
- ðŸ”³ Create integration tests for full data pipeline
- ðŸ”³ Add local test runner script for comprehensive validation

**Deliverables:**
- Comprehensive local validation suite
- Minimal GitHub Actions workflow (PRs to main only)
- Manual validation checklist and PR templates
- Data quality reports and documentation

---

### Phase 4: Initial Data Population
**Goal:** Create first production dataset (v1.0.0)

#### 4.1: Category Structure
- ðŸ”³ Define top-level categories (10-15 major categories)
- ðŸ”³ Create subcategory hierarchy (2-3 levels deep)
- ðŸ”³ Assign icons/symbols to each category
- ðŸ”³ Set sort order for logical browsing
- ðŸ”³ Create category JSON files
- ðŸ”³ Document category taxonomy

#### 4.2: Item Data Collection
- ðŸ”³ Run Goodwill scraper and collect data
- ðŸ”³ Run Salvation Army parser and collect data
- ðŸ”³ Parse IRS Publication 561 examples
- ðŸ”³ Normalize all collected data
- ðŸ”³ Merge valuations from multiple sources
- ðŸ”³ Calculate recommended values
- ðŸ”³ Organize items into category folders
- ðŸ”³ Target: 300-500 items for v1.0.0

#### 4.3: Metadata Files
- ðŸ”³ Create sources.json with full attribution
- ðŸ”³ Create condition-terms.json (IRS-approved terms)
- ðŸ”³ Create valuation-methods.json
- ðŸ”³ Document data collection dates
- ðŸ”³ Add source URLs and access dates

#### 4.4: Version 1.0.0 Release
- ðŸ”³ Run full validation suite
- ðŸ”³ Generate version.json file
- ðŸ”³ Create CHANGELOG entry
- ðŸ”³ Archive source documents in archive/2024-Q1/
- ðŸ”³ Tag release as v1.0.0
- ðŸ”³ Create GitHub release with notes
- ðŸ”³ Update README with usage examples

**Deliverables:**
- Production-ready v1.0.0 dataset
- 300-500 items across major categories
- Full source attribution and documentation

---

### Phase 5: App Integration Support
**Goal:** Ensure data format works seamlessly with app

#### 5.1: Data Access Testing
- ðŸ”³ Test GitHub raw URL access for all files
- ðŸ”³ Verify CORS headers allow app access
- ðŸ”³ Test version.json download and parsing
- ðŸ”³ Test category file downloads
- ðŸ”³ Test item file downloads
- ðŸ”³ Measure download sizes and performance
- ðŸ”³ Test with slow network conditions

#### 5.2: Sample Integration Code
- ðŸ”³ Create example Swift code for downloading data
- ðŸ”³ Create example parsing code
- ðŸ”³ Document expected data structures
- ðŸ”³ Provide sample merge logic
- ðŸ”³ Add to README for app developers

#### 5.3: API Documentation
- ðŸ”³ Document all JSON schemas
- ðŸ”³ Create API reference for data access
- ðŸ”³ Document versioning strategy
- ðŸ”³ Explain update/merge process
- ðŸ”³ Provide migration guides for schema changes

**Deliverables:**
- Verified data access from app
- Integration documentation
- Sample code and examples

---

### Phase 6: Maintenance & Update Workflow
**Goal:** Establish sustainable update process

#### 6.1: Update Procedures
- ðŸ”³ Document quarterly update schedule
- ðŸ”³ Create checklist for data updates
- ðŸ”³ Define version numbering scheme (semantic versioning)
- ðŸ”³ Create update workflow diagram
- ðŸ”³ Document rollback procedures
- ðŸ”³ Create data diff tools to show changes

#### 6.2: Automation
- ðŸ”³ Create script to run all scrapers in sequence
- ðŸ”³ Automate data normalization pipeline
- ðŸ”³ Automate validation checks
- ðŸ”³ Auto-generate version.json from data
- ðŸ”³ Create release preparation script
- ðŸ”³ Set up scheduled GitHub Actions for scraping

#### 6.3: Monitoring
- ðŸ”³ Set up alerts for scraper failures
- ðŸ”³ Monitor source website changes
- ðŸ”³ Track data quality metrics over time
- ðŸ”³ Log download statistics (if possible)
- ðŸ”³ Monitor GitHub issues for data corrections

**Deliverables:**
- Documented update workflow
- Automated update pipeline
- Monitoring and alerting

---

### Phase 7: Community & Governance
**Goal:** Enable community contributions while maintaining quality

#### 7.1: Contribution Guidelines
- ðŸ”³ Create CONTRIBUTING.md
- ðŸ”³ Define data submission standards
- ðŸ”³ Create PR template for data updates
- ðŸ”³ Document review process
- ðŸ”³ Set up code owners for data files
- ðŸ”³ Create issue templates for corrections

#### 7.2: Community Features
- ðŸ”³ Enable GitHub Discussions for questions
- ðŸ”³ Create FAQ document
- ðŸ”³ Set up wiki for extended documentation
- ðŸ”³ Create examples of common contributions
- ðŸ”³ Recognize contributors in CHANGELOG

#### 7.3: Legal & Compliance
- ðŸ”³ Add legal disclaimer to README
- ðŸ”³ Document data provenance for all items
- ðŸ”³ Maintain copies of source documents
- ðŸ”³ Create audit trail documentation
- ðŸ”³ Review and update license as needed
- ðŸ”³ Ensure no proprietary Intuit data included

**Deliverables:**
- Open contribution process
- Legal compliance documentation
- Active community engagement

---

## Quarterly Maintenance Process

### Pre-Update (Week 1)
1. Check source websites for updates
2. Download latest source documents
3. Archive current data version
4. Create new branch for update

### Data Collection (Week 2)
1. Run all scraping scripts
2. Parse new data
3. Compare with previous version
4. Flag significant changes (>20% variance)
5. Manual review of flagged items

### Validation & Testing (Week 3)
1. Run validation suite
2. Check data quality metrics
3. Test sample downloads
4. Review community-reported issues
5. Make corrections as needed

### Release (Week 4)
1. Update version.json
2. Update CHANGELOG.md
3. Create GitHub release
4. Tag with version number
5. Announce update to community
6. Monitor for issues

## Success Metrics

### Data Quality
- **Coverage**: 500+ items across 20+ categories
- **Accuracy**: <5% error rate in valuations
- **Freshness**: Updated within 30 days of source changes
- **Completeness**: >95% of items have all required fields

### Repository Health
- **Validation**: 100% of data passes validation
- **Documentation**: All schemas documented
- **Tests**: >90% code coverage in scripts
- **Issues**: <7 day average response time

### App Integration
- **Download Success**: >99% success rate
- **Performance**: <5 seconds for full data download
- **Compatibility**: Works with app versions 1.0+

## Risk Management

### Technical Risks
| Risk | Impact | Mitigation |
|------|--------|------------|
| Source website changes | High | Monitor sites, maintain scrapers, manual fallback |
| Data corruption | High | Validation scripts, version control, backups |
| GitHub downtime | Medium | App caches data locally, graceful degradation |
| Breaking schema changes | High | Semantic versioning, migration guides |

### Legal Risks
| Risk | Impact | Mitigation |
|------|--------|------------|
| Copyright claims | High | Only use public sources, full attribution |
| Patent infringement | High | Avoid Intuit-specific methods, document differences |
| Data accuracy liability | Medium | Disclaimer, cite sources, community review |

### Operational Risks
| Risk | Impact | Mitigation |
|------|--------|------------|
| Maintainer availability | Medium | Document all processes, automate where possible |
| Community spam/vandalism | Low | PR reviews, branch protection, code owners |
| Scraper failures | Medium | Error handling, alerts, manual backup process |

## Timeline

### Immediate (Weeks 1-2)
- Phase 1: Repository setup
- Phase 2.1: Goodwill scraper

### Short-term (Weeks 3-6)
- Phase 2.2-2.4: Complete data collection
- Phase 3: Validation and testing
- Phase 4.1-4.2: Initial data population

### Medium-term (Weeks 7-10)
- Phase 4.3-4.4: v1.0.0 release
- Phase 5: App integration support
- Phase 6.1: Update procedures

### Long-term (Weeks 11+)
- Phase 6.2-6.3: Automation and monitoring
- Phase 7: Community features
- Ongoing: Quarterly updates

## Next Steps

**Before starting development, please review and confirm:**

1. **Repository naming**: Is `myWriteOffs-data` the correct name?
2. **GitHub organization**: Should this be under a personal account or organization?
3. **License choice**: CC0 (public domain) or MIT for data files?
4. **Initial scope**: Start with 300-500 items or aim higher?
5. **Priority categories**: Which categories are most important for v1.0.0?
6. **Scraping approach**: Automated scraping vs. manual data entry for first version?
7. **Update frequency**: Quarterly updates acceptable or prefer different cadence?

**Recommended starting point:**
- Begin with **Phase 1** (repository setup)
- Then **Phase 2.1** (Goodwill scraper) as proof of concept
- Validate approach before scaling to all sources

This plan is designed to be iterative - we can adjust based on what works and what doesn't as we progress.
